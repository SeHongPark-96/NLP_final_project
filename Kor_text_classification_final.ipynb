{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kor_text_classification_final.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "rmlw0X4Y9Sy3",
        "9_5cmL9GeltW",
        "RrI1bA2Uem9I",
        "r4vGkZNOBiRl",
        "TdEYU5FeT0xa",
        "LtwhRoNTe6_Y",
        "gH1HIaU19eNF",
        "YktxhH_I9iKQ",
        "ell56TNOh2-U"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1fZQjbndlIxST0Yrx4afJiiSuyIxGbKTj",
      "authorship_tag": "ABX9TyOamixaNTlj4f4kgX3k9JEy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeHongPark-96/NLP_final_project/blob/main/Kor_text_classification_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 필요 라이브러리 호출"
      ],
      "metadata": {
        "id": "2QcNSx5B7osG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3A9P8hWcMPo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from tqdm import tqdm_notebook\n",
        "from matplotlib import rcParams\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as tfl\n",
        "from tensorflow.keras import Model\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import gensim\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/자연어처리/실습/기말고사_대체과제')"
      ],
      "metadata": {
        "id": "2eQVCZ420uMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/자연어처리/실습/기말고사_대체과제/ttrain.csv', header=1)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-F4a8ceZnXvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 파악"
      ],
      "metadata": {
        "id": "ghCKpAdY8iEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 상대적으로 사회 topic이 적음\n",
        "print('topic : ', df['label'].unique())\n",
        "df.groupby(df['label']).size()"
      ],
      "metadata": {
        "id": "6IsHh5HOeusM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist([len(title) for title in df['title']])\n",
        "print('뉴스 갯수 : ', len(df))\n",
        "print('뉴스 제목 최대 길이 : ', max(len(t) for t in df['title']))\n",
        "print('뉴스 제목 평균 길이 : ', sum(map(len, df['title']))/ len(df['title']))"
      ],
      "metadata": {
        "id": "jE-ofKNjcTDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E26dWdH-wAMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('title null값 : ', sum(df['title'].isnull()))\n",
        "print('label null값 : ', sum(df['label'].isnull()))"
      ],
      "metadata": {
        "id": "73nfnfQ4oeCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글 외의 단어들 파악 - 주로 특수기호, 한자 혹은 영어로 된 기업명 등의 고유명사들\n",
        "def get_non_ko(title_list):\n",
        "\n",
        "    non_ko_words = []\n",
        "    non_ko = re.compile('[ ㄱ-ㅣ가-힣0-9.…·+]')\n",
        "    # non_ko = re.compile('[^ ㄱ-ㅣ가-힣A-Za-z]')\n",
        "\n",
        "\n",
        "    for title in title_list:\n",
        "        non_ko_word = non_ko.sub('', title)\n",
        "        if len(non_ko_word) >= 1 :\n",
        "            non_ko_words.append(non_ko_word)\n",
        "\n",
        "    return non_ko_words"
      ],
      "metadata": {
        "id": "25bF4rzn-2jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_non_ko(df['title'])"
      ],
      "metadata": {
        "id": "uUzVBHq2Aaqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 전처리"
      ],
      "metadata": {
        "id": "qARTK4zh8ngj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. 정규표현식으로 기본적인 전처리"
      ],
      "metadata": {
        "id": "gBlSpzn5EYx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "percent = '오늘은 증시가 510123만원 더 올랐다'\n",
        "numeric_value = re.compile(r'[0-9가-힣]*원')\n",
        "print(numeric_value.sub('수치', percent))"
      ],
      "metadata": {
        "id": "uwK-HABsEYVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 특수문자 제거\n",
        "def get_clean_words(title_list):\n",
        "\n",
        "    clean_words = []\n",
        "    numeric_value = re.compile(r'[0-9]*%') # 숫자 + %는 주제 판별에 유의미할 수 있기 때문에 \"수치\"로 남김\n",
        "    currency = re.compile(r'[0-9가-힣]*원')\n",
        "    non_word = re.compile('[^ ㄱ-ㅣ가-힣]')\n",
        "\n",
        "\n",
        "    for title in title_list:\n",
        "        word = numeric_value.sub(' 수치', title)\n",
        "        word = currency.sub(' 금액', word)\n",
        "        word = non_word.sub(' ', word)\n",
        "        clean_words.append(word)\n",
        "\n",
        "    return clean_words"
      ],
      "metadata": {
        "id": "Y_zNlMhwCEOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_title'] = get_clean_words(df['title'])\n",
        "df"
      ],
      "metadata": {
        "id": "Z0cEENoICd0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mecab 활용"
      ],
      "metadata": {
        "id": "8m3qzl8n91r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git # 필요한 파일 clone\n",
        "# os.chdir('./Mecab-ko-for-Google-Colab')\n",
        "\n",
        "# ! bash install_mecab-ko_on_colab190912.sh\n",
        "# os.chdir('../')\n"
      ],
      "metadata": {
        "id": "3ulYJYR38nNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from konlpy.tag import Mecab\n",
        "# mecab = Mecab()"
      ],
      "metadata": {
        "id": "vdczJ9pS9QEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_words_mecab = ['아니', '이달', '계속', '내달', '따라', '내년','올해', \n",
        "#               '주말', '내일', '주년', '개월', '오늘', '위해', '오후', '위한', \n",
        "#               '다음', '일부', '이후', '대신', '만나', '이번', '하루', \n",
        "#               '앞둔', '만들', '첫날', '천만', '아냐', '누구', '사실', \n",
        "#               '오전', '천억', '지난해', '잇단' ]"
      ],
      "metadata": {
        "id": "46qK_WvlVYEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_nv_mecab(title):\n",
        "#     clean_title = []\n",
        "#     result = mecab.pos(title)\n",
        "#     for word, tag in result:\n",
        "#         if (tag.startswith('NNG') or tag.startswith('SL')) and len(word)>1 and word not in stop_words_mecab:\n",
        "#             clean_title.append(word)\n",
        "\n",
        "#     return \" \".join(clean_title)"
      ],
      "metadata": {
        "id": "se54o0Wz97C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def print_input_output(title):\n",
        "#     print('기존 제목 : ', title)\n",
        "#     print('mecab 적용 후 : ', extract_nv_mecab(title))"
      ],
      "metadata": {
        "id": "RPQLw3Fy-cQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mecab.pos(df['cleaned_title'][3])"
      ],
      "metadata": {
        "id": "IPcS3LyK1Qrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# title_tokenized = []\n",
        "\n",
        "# for title in tqdm(df['cleaned_title']):\n",
        "#     try:\n",
        "#         title_tokenized.append(extract_nv_mecab(title))\n",
        "\n",
        "#     except:\n",
        "#         title_tokenized.append(title)\n",
        "#         print(sent)\n",
        "\n",
        "# df['tokenized_title_mecab'] = title_tokenized\n",
        "\n",
        "# df\n"
      ],
      "metadata": {
        "id": "SFapkBEMLUva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.hist([len(title) for title in df['tokenized_title_mecab']], bins=5)\n",
        "# print('뉴스 갯수 : ', len(df))\n",
        "# print('전처리 이후 뉴스 제목 최대 길이 : ', max(len(t) for t in df['tokenized_title_mecab']))\n",
        "# print('전처리 이후 뉴스 제목 평균 길이 : ', sum(map(len, df['tokenized_title_mecab']))/ len(df['tokenized_title_mecab']))"
      ],
      "metadata": {
        "id": "hhEc1_FEMazB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(df[[len(x)>=42 for x in df['tokenized_title_mecab']]])"
      ],
      "metadata": {
        "id": "ZXbmhce_SOtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Okt 활용"
      ],
      "metadata": {
        "id": "YCGkrwhJYEqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "D0ubUNJwYuJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()"
      ],
      "metadata": {
        "id": "J5i_Z9PtYuGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def okt_result(title):\n",
        "    print('기존 제목 : ', title)\n",
        "    print('Okt 적용 결과 : ', okt.pos(title, norm=True, stem=True))"
      ],
      "metadata": {
        "id": "EZCVFHWDYuDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt_result(df['cleaned_title'][0])"
      ],
      "metadata": {
        "id": "_5w9b_ufYuA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_okt = ['하다', '으로', '되다', '만에', '없다', '에서', '까지', \n",
        "                  '부터', '올해', '대다', '있다', '작년', '않다', '돼다', \n",
        "                  '에도', '내년', '맞다', '오늘', '내일', '주말', '이다', \n",
        "                  '내달', '주년', '번째', '개월', '위해', '에게', '오후', \n",
        "                  '다시', '함께', '아니다', '하고', '이후', '이틀', '대신', \n",
        "                  '내다', '일부', '없이', '싶다', '첫날', '처럼', '오전',\n",
        "                  '멀리', '가장', '종합']"
      ],
      "metadata": {
        "id": "6H8z_HwRmcBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_nv_okt(title):\n",
        "    clean_title = []\n",
        "    result = okt.pos(title, norm=True, stem=True)\n",
        "\n",
        "    for word, tag in result:\n",
        "        if (tag=='Noun' or tag == 'Adjective' or tag=='Verb' or tag == 'KoreanParticle' or tag=='Adverb') and len(word)>1 and word not in stop_words_okt:\n",
        "            clean_title.append(word)\n",
        "\n",
        "    return ' '.join(clean_title)"
      ],
      "metadata": {
        "id": "Z4GsYW37ZNNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt_title_train = []\n",
        "\n",
        "for title in tqdm(df['cleaned_title']):\n",
        "    processed_w = extract_nv_okt(title)\n",
        "\n",
        "    okt_title_train.append(processed_w)\n",
        "\n",
        "df['tokenized_title_okt'] = okt_title_train\n",
        "df"
      ],
      "metadata": {
        "id": "N-F3AaBDZNKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist([len(title) for title in df['tokenized_title_okt']], bins=5)\n",
        "print('뉴스 갯수 : ', len(df))\n",
        "print('전처리 이후 뉴스 제목 최대 길이 : ', max(len(t) for t in df['tokenized_title_okt']))\n",
        "print('전처리 이후 뉴스 제목 평균 길이 : ', sum(map(len, df['tokenized_title_okt']))/ len(df['tokenized_title_okt']))"
      ],
      "metadata": {
        "id": "380MLMgabyMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텍스트로 되어있는 label int로 인코딩"
      ],
      "metadata": {
        "id": "S1FXsRmX7vZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics = {'IT과학' : '0', '생활문화' : '1', \n",
        "          '스포츠' : '2', '사회' : '3', \n",
        "          '세계' : '4', '정치' : '5', '경제' : '6'}\n",
        "\n",
        "df['label'] = df['label'].map(lambda x: topics.get(x,x))\n",
        "df"
      ],
      "metadata": {
        "id": "U0Qxb9T_P69d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 형태소 분석 결과 확인"
      ],
      "metadata": {
        "id": "u4Zxb9pNbS7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # mecab 결과\n",
        "\n",
        "# # counter = Counter(word for title in df['tokenized_title_mecab'] for word in title.split())\n",
        "# # counter = {word : frequency for word, frequency in counter.items() if frequency >= 10}\n",
        "# print(sorted(counter.items(), key=lambda item:item[1], reverse=True))\n",
        "# print(len(counter))"
      ],
      "metadata": {
        "id": "yyWH6L5bPsBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#okt 결과 \n",
        "\n",
        "counter = Counter(word for title in df['tokenized_title_okt'] for word in title.split())\n",
        "counter = {word : frequency for word, frequency in counter.items() if frequency >= 10}\n",
        "print(sorted(counter.items(), key=lambda item:item[1], reverse=True))\n",
        "print(len(counter))\n",
        "\n",
        "num_words=len(counter)"
      ],
      "metadata": {
        "id": "xk8Bsu-HbkR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 분리"
      ],
      "metadata": {
        "id": "rmlw0X4Y9Sy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "train_label = to_categorical(df['label'])\n",
        "train_label"
      ],
      "metadata": {
        "id": "Mt4efd1m9vB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 분리\n",
        "# max_length = 41\n",
        "# df['label'] = np_utils.to_categorical(df['label'])\n",
        "training_titles, validation_titles, training_labels , validation_labels = train_test_split(df['tokenized_title_okt'], train_label,\n",
        "                                                                                           stratify = train_label, shuffle=True,\n",
        "                                                                                           test_size=0.15, random_state=0)\n",
        "\n",
        "\n",
        "print(len(training_titles))"
      ],
      "metadata": {
        "id": "Ch73eVdPXhHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델링 및 학습"
      ],
      "metadata": {
        "id": "9_5cmL9GeltW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. keras 활용 DNN (okt)"
      ],
      "metadata": {
        "id": "RrI1bA2Uem9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 단어 기반"
      ],
      "metadata": {
        "id": "r4vGkZNOBiRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# num_words = 1000"
      ],
      "metadata": {
        "id": "LTuya_mq2ms1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenizer = Tokenizer(num_words=num_words, oov_token='<OOV>')\n",
        "word_tokenizer.fit_on_texts(training_titles)\n",
        "word_index = word_tokenizer.word_index\n",
        "\n",
        "print(len(word_index))"
      ],
      "metadata": {
        "id": "TK9XH8wXBk2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max(([len(x.split()) for x in df['tokenized_title_mecab']]))"
      ],
      "metadata": {
        "id": "4n4lCLne2Aro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 13\n",
        "word_training_sequences = word_tokenizer.texts_to_sequences(training_titles)\n",
        "word_training_padded = pad_sequences(word_training_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "word_validation_sequences = word_tokenizer.texts_to_sequences(validation_titles)\n",
        "word_validation_padded = pad_sequences(word_validation_sequences, maxlen=max_length, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "s8LZTJ4zBmkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_training_padded.shape)\n",
        "print(word_validation_padded.shape)"
      ],
      "metadata": {
        "id": "-TMmG6EEBmfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 단어 기반 모델 model"
      ],
      "metadata": {
        "id": "TdEYU5FeT0xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conv1D\n",
        "word_dim = 32\n",
        "\n",
        "word_model = tf.keras.Sequential([\n",
        "                             tfl.Embedding(input_dim=num_words, output_dim=word_dim, input_length=max_length),\n",
        "                             tfl.Dropout(0.2),\n",
        "                             tfl.Conv1D(32, 3, padding='same', activation='relu'),\n",
        "                             tfl.GlobalMaxPooling1D(),\n",
        "                             tfl.Dense(250, activation='relu'),\n",
        "                             tfl.Dropout(0.2),\n",
        "                             tfl.Dense(7, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "word_model.summary()"
      ],
      "metadata": {
        "id": "su7gFqv7G7kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN\n",
        "word_dim = 32\n",
        "\n",
        "word_model = tf.keras.Sequential([\n",
        "                             tfl.Embedding(input_dim=num_words, output_dim=word_dim, input_length=max_length),\n",
        "                            #  tfl.GRU(64, return_sequences=True),\n",
        "                             tfl.Bidirectional(tfl.GRU(32, return_sequences=True)),\n",
        "                             tfl.GlobalMaxPooling1D(),\n",
        "                             tfl.Dropout(0.4),\n",
        "                            #  tfl.Dense(16, activation='relu'),\n",
        "                            #  tfl.Dropout(0.3),\n",
        "                             tfl.Dense(7, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "word_model.summary()"
      ],
      "metadata": {
        "id": "jRFmf9HTT0aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate = 0.001,\n",
        "    decay_steps = 100,\n",
        "    decay_rate = 0.96,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "word_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr_schedule), metrics=['accuracy'])\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, \n",
        "                                                  restore_best_weights=True, verbose=1)\n",
        "\n",
        "history = word_model.fit(word_training_padded, training_labels,\n",
        "                    epochs=100, verbose=2, batch_size=256, shuffle=True,\n",
        "                    validation_data=(word_validation_padded, validation_labels),\n",
        "                    callbacks = [early_stopping])"
      ],
      "metadata": {
        "id": "oYbp4qLXT7Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Tf-Idf"
      ],
      "metadata": {
        "id": "LtwhRoNTe6_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tfidf 벡터화"
      ],
      "metadata": {
        "id": "gH1HIaU19eNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=5, ngram_range=(1,2),\n",
        "                             max_features=num_words)\n",
        "\n",
        "tfidf_train = vectorizer.fit_transform(training_titles).todense()\n",
        "tfidf_validation = vectorizer.transform(validation_titles).todense()"
      ],
      "metadata": {
        "id": "ouV1_7C1e9OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 모델링"
      ],
      "metadata": {
        "id": "YktxhH_I9iKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_model = tf.keras.Sequential([\n",
        "                             tfl.Dense(256,input_shape = (num_words,)),\n",
        "                            #  tfl.BatchNormalization(),\n",
        "                             tfl.Activation('relu'),\n",
        "                             tfl.Dropout(0.5),\n",
        "\n",
        "                            #  tfl.Dense(128),\n",
        "                            #  tfl.BatchNormalization(),\n",
        "                            #  tfl.Activation('relu'),\n",
        "                            #  tfl.Dropout(0.3),\n",
        "\n",
        "                             tfl.Dense(64),\n",
        "                            #  tfl.BatchNormalization(),\n",
        "                             tfl.Activation('relu'),\n",
        "                             tfl.Dropout(0.3),\n",
        "                             \n",
        "                             tfl.Dense(7, activation='softmax')\n",
        "])\n",
        "lr_shedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate = 1e-2,\n",
        "    decay_steps = 100,\n",
        "    decay_rate = 0.96,\n",
        "    staircase=True\n",
        ")\n",
        "tf_idf_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr_schedule), metrics=['accuracy'])\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, \n",
        "                                                  restore_best_weights=True, verbose=1)\n",
        "\n",
        "history = tf_idf_model.fit(tfidf_train, training_labels,\n",
        "                    epochs=100, verbose=2, batch_size=128,\n",
        "                    validation_data=(tfidf_validation, validation_labels),\n",
        "                    callbacks = [early_stopping])"
      ],
      "metadata": {
        "id": "K5cAP81agFT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 평가 \n",
        "- ttrain을 train / validation으로 나누어 진행했을시, 80%의 성능을 보였지만, ttrain으로 학습 후 ttest에 적용했을때 60 후반대의 성능 밖에 안나옴. ttest의 단어들이 ttrain에 없는 경우들이 상당히 나옴. \n",
        "- 제목이 적은 수의 단어로 이루어진 경우들이 있어 충분한 학습 데이터 부족으로 성능의 한계가 보임.\n",
        "- 추가적인 데이터 수집이 필요해보임 "
      ],
      "metadata": {
        "id": "CdcokMGH1RdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test 데이터 불러오기"
      ],
      "metadata": {
        "id": "Y6M3Fv-WCdJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/자연어처리/실습/기말고사_대체과제/ttest.csv', header=1)\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "3LxkDTSD1RNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['cleaned_title'] = get_clean_words(df_test['title'])\n",
        "df_test"
      ],
      "metadata": {
        "id": "-nF9wkphLPYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_title_tokenized = []\n",
        "\n",
        "# for title in tqdm(df_test['cleaned_title']):\n",
        "#     try:\n",
        "#         test_title_tokenized.append(extract_nv_mecab(title))\n",
        "\n",
        "#     except:\n",
        "#         test_title_tokenized.append(title)\n",
        "#         print(title)\n",
        "\n",
        "# df_test['tokenized_title_mecab'] = test_title_tokenized\n",
        "\n",
        "# df_test"
      ],
      "metadata": {
        "id": "frYjrvjFLlE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt_title_test = []\n",
        "\n",
        "for title in tqdm(df_test['cleaned_title']):\n",
        "    processed_w = extract_nv_okt(title)\n",
        "\n",
        "    okt_title_test.append(processed_w)\n",
        "\n",
        "df_test['tokenized_title_okt'] = okt_title_test\n",
        "# df_test"
      ],
      "metadata": {
        "id": "AHhtxV-Acg8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['label'] = df_test['label'].map(lambda x: topics.get(x,x))\n",
        "df_test"
      ],
      "metadata": {
        "id": "hNbXsY560hvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### keras"
      ],
      "metadata": {
        "id": "CD8sSguuqfmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# validation을 위해 나눴던 것과 다르게 test 예측에는 모두 활용\n",
        "max_length = 14\n",
        "all_word_tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "all_word_tokenizer.fit_on_texts(df['tokenized_title_okt'])\n",
        "all_word_sequences = all_word_tokenizer.texts_to_sequences(df['tokenized_title_okt'])\n",
        "all_word_padded = pad_sequences(all_word_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "all_word_labels = to_categorical(df['label'])"
      ],
      "metadata": {
        "id": "BKf380ZyfxOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_word_tokenizer.word_index)"
      ],
      "metadata": {
        "id": "HFRK7mE5AhbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_test_sequences = all_word_tokenizer.texts_to_sequences(df_test['tokenized_title_okt'])\n",
        "word_test_padded = pad_sequences(word_test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "test_labels = to_categorical(df_test['label'])"
      ],
      "metadata": {
        "id": "UlUC7sytfyND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "id": "88WThKiFix69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_word_tokenizer.sequences_to_texts(word_test_sequences)"
      ],
      "metadata": {
        "id": "4IC-hwPmAYYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_words= 7500\n",
        "\n",
        "word_dim = 200\n",
        "\n",
        "word_model = tf.keras.Sequential([\n",
        "                             tfl.Embedding(input_dim=num_words, output_dim=word_dim, input_length=max_length),\n",
        "                             tfl.Dropout(0.2),\n",
        "                             tfl.Conv1D(32, 3, padding='same', activation='relu'),\n",
        "                             tfl.GlobalMaxPooling1D(),\n",
        "                             tfl.Dense(32, activation='relu'),\n",
        "                             tfl.Dropout(0.2),\n",
        "                             tfl.Dense(7, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "word_model.summary()\n",
        "\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate = 0.001,\n",
        "    decay_steps = 100,\n",
        "    decay_rate = 0.96,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "word_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr_schedule), metrics=['accuracy'])\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, \n",
        "                                                  restore_best_weights=True, verbose=1)\n",
        "\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, \n",
        "#                                                   restore_best_weights=True, verbose=1)\n",
        "\n",
        "history = word_model.fit(all_word_padded, all_word_labels,\n",
        "                    epochs=20, verbose=2, batch_size=128, shuffle=True,\n",
        "                    validation_data = (word_test_padded, test_labels),\n",
        "                    callbacks = [early_stopping])"
      ],
      "metadata": {
        "id": "LfvQ7bS4fl4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_model.evaluate(word_test_padded, test_labels)"
      ],
      "metadata": {
        "id": "Wd6ql9Vzgt--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tfidf"
      ],
      "metadata": {
        "id": "ell56TNOh2-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "final_vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
        "                             max_features=num_words)\n",
        "\n",
        "tfidf_final = final_vectorizer.fit_transform(df['tokenized_title_mecab']).todense()\n"
      ],
      "metadata": {
        "id": "L0rqHDyFkhNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_test = final_vectorizer.transform(df_test['tokenized_title_mecab']).todense()\n",
        "test_labels = to_categorical(df_test['label'])"
      ],
      "metadata": {
        "id": "LZD68Av2h2ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_model = tf.keras.Sequential([\n",
        "                             tfl.Dense(128,input_shape = (num_words,)),\n",
        "                            #  tfl.BatchNormalization(),\n",
        "                             tfl.Activation('relu'),\n",
        "                             tfl.Dropout(0.4),\n",
        "\n",
        "                            #  tfl.Dense(128),\n",
        "                            #  tfl.BatchNormalization(),\n",
        "                            #  tfl.Activation('relu'),\n",
        "                            #  tfl.Dropout(0.3),\n",
        "\n",
        "                             tfl.Dense(32),\n",
        "                            #  tfl.BatchNormalization(),\n",
        "                             tfl.Activation('relu'),\n",
        "                             tfl.Dropout(0.4),\n",
        "                             \n",
        "                             tfl.Dense(7, activation='softmax')\n",
        "])\n",
        "lr_shedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate = 1e-2,\n",
        "    decay_steps = 100,\n",
        "    decay_rate = 0.96,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=5,\n",
        "                                                  restore_best_weights=True, verbose=1)\n",
        "\n",
        "tf_idf_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr_schedule), metrics=['accuracy'])\n",
        "\n",
        "history = tf_idf_model.fit(tfidf_final, all_word_labels,\n",
        "                    epochs=20, verbose=2, batch_size=128,\n",
        "                    shuffle=True, \n",
        "                    validation_data=(tfidf_test, test_labels),\n",
        "                    callbacks = [early_stopping])"
      ],
      "metadata": {
        "id": "oNoZFEu0k0r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_model.evaluate(tfidf_test, test_labels)"
      ],
      "metadata": {
        "id": "n9MEC6Ufk0ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML 적용"
      ],
      "metadata": {
        "id": "Q1OFcFiqlm3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# labels = df['label']\n",
        "# test_labels = df_test['label']"
      ],
      "metadata": {
        "id": "TzpPL5nAl9bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.svm import LinearSVC\n",
        "# from sklearn import metrics\n",
        "# svm = LinearSVC(C=1)\n",
        "# svm.fit(tfidf_final, labels)\n",
        "# pred = svm.predict(tfidf_test)\n",
        "\n",
        "# print(metrics.accuracy_score(test_labels, pred))"
      ],
      "metadata": {
        "id": "dpiM6Jz3k0l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 추가) 사전 학습된 모델 사용하기"
      ],
      "metadata": {
        "id": "4rwFMOGrxhbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KoBert\n",
        "- KoBert 사용시 정확도가 0.8까지 올라가는 것을 보아 학습 데이터 부족의 문제도 있음을 확인\n",
        "\n",
        "출처 : https://github.com/SKTBrain/KoBERT"
      ],
      "metadata": {
        "id": "A8q9GfvDnC3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "metadata": {
        "id": "5XmNtjT3TK5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "import gluonnlp as nlp\n"
      ],
      "metadata": {
        "id": "0lqwabLtTWPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0')"
      ],
      "metadata": {
        "id": "r14c_PxDTkJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "metadata": {
        "id": "Q8ANuNyWTpBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 14\n",
        "batch_size = 128\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate = 5e-5"
      ],
      "metadata": {
        "id": "RYB6XDbnT3M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = nlp.data.TSVDataset('/content/drive/MyDrive/자연어처리/실습/기말고사_대체과제/df.tsv', field_indices=[1,0], num_discard_samples=1)\n",
        "dataset_test = nlp.data.TSVDataset('/content/drive/MyDrive/자연어처리/실습/기말고사_대체과제/df_test.tsv', field_indices=[1,0], num_discard_samples=1)"
      ],
      "metadata": {
        "id": "iHG-OTbheKzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "metadata": {
        "id": "8XU5XUzqWdsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ],
      "metadata": {
        "id": "xxu_IFYqUKDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = BERTDataset(dataset_train, 0, 1, tok, max_length, True, False)\n",
        "dataset_test = BERTDataset(dataset_test, 0, 1, tok, max_length, True, False)"
      ],
      "metadata": {
        "id": "ChQsxlbeV4Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#배치 및 데이터로더 설정\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, num_workers=4)"
      ],
      "metadata": {
        "id": "lgIJ5LtmZL5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=7, ##주의: 클래스 수 바꾸어 주세요!##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "metadata": {
        "id": "bi0TGgJlWKEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)"
      ],
      "metadata": {
        "id": "6l19MmQOY-g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
      ],
      "metadata": {
        "id": "oKpYfvX6ZFza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ],
      "metadata": {
        "id": "FdhY7GYZZJEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} validation acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ],
      "metadata": {
        "id": "l2O47XC0ZTjd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}